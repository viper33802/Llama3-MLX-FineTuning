{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1469cd84-1952-4d82-882f-d2a877ca6a5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instal Requirements\n",
    "\n",
    "!pip install mlx-lm transformers datasets torch TensorFlow huggingface_hub ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76792fa2-a0bb-4be6-a582-0d086938c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@anchen.li/fine-tune-llama3-with-function-calling-via-mlx-lm-5ebbee41558f\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import mlx_lm\n",
    "import ipywidgets\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import login\n",
    "\n",
    "proj_name=\"MyProject\"\n",
    "HFAPI=\"<HF_API_KEY>\"\n",
    "llamacpp_path=\"/Path/To/llama.cpp/\"\n",
    "ds_repo_id = \"mzbac/function-calling-llama-3-format-v1.1\"\n",
    "hf_model=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "proj_dir=f\"/Users/xxxxxxxx/projects/{proj_name}/\"\n",
    "model_dir=f\"{proj_dir}model/\"\n",
    "adapters_dir=f\"{proj_dir}model/adapters/\"\n",
    "data_dir=f\"{proj_dir}data/\"\n",
    "fused_dir=f\"{proj_dir}model/fused/\"\n",
    "lora_config=f\"{proj_dir}model/lora_config.yaml\"\n",
    "gguf_file=f\"{proj_name}_fp16.GGUF\"\n",
    "out_gguf=f\"{proj_dir}model/{gguf_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939df742-2efe-4686-923b-46f9ef2d9e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm created variables\n",
    "\n",
    "print(proj_dir)\n",
    "print(adapters_dir)\n",
    "print(data_dir)\n",
    "print(fused_dir)\n",
    "print(lora_config)\n",
    "print(gguf_file)\n",
    "print(out_gguf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c00399-6a3b-4b87-a65b-77f11de53327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "\n",
    "login(HFAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c67d55d-cc23-43a3-86b0-18fd1be23a2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup Project directory structure\n",
    "\n",
    "os.mkdir(proj_dir)\n",
    "os.chdir(proj_dir)\n",
    "os.mkdir(model_dir)\n",
    "os.mkdir(adapters_dir)\n",
    "os.mkdir(data_dir)\n",
    "os.mkdir(fused_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07b6659-6dfb-423b-881e-be5cba33b494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download the training dataset\n",
    "# Can optionaly copy file into data folder\n",
    "\n",
    "!huggingface-cli download {ds_repo_id} --repo-type dataset --include \"*.jsonl\" --local-dir={data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c11ac-feef-4e60-89c0-c50810d13eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LoRA configuration file\n",
    "# !!Review closely for finetuning settings!!\n",
    "\n",
    "yaml_content = {\n",
    "    \"model\": hf_model,  # The path to the local model directory or Hugging Face repo.\n",
    "    \"train\": True,  # Whether or not to train (boolean)\n",
    "    \"data\": data_dir,  # Directory with {train, valid, test}.jsonl files\n",
    "    \"seed\": 0,  # The PRNG seed\n",
    "    \"lora_layers\": 32,  # Number of layers to fine-tune\n",
    "    \"batch_size\": 1,  # Minibatch size.\n",
    "    \"iters\": 100,  # Iterations to train for.\n",
    "    \"val_batches\": 25,  # Number of validation batches, -1 uses the entire validation set.\n",
    "    \"learning_rate\": 1e-6,  # Adam learning rate.\n",
    "    \"steps_per_report\": 10,  # Number of training steps between loss reporting.\n",
    "    \"steps_per_eval\": 200,  # Number of training steps between validations.\n",
    "    \"resume_adapter_file\": None,  # Load path to resume training with the given adapter weights.\n",
    "    \"adapter_path\": adapters_dir,  # Save/load path for the trained adapter weights.\n",
    "    \"save_every\": 1000,  # Save the model every N iterations.\n",
    "    \"test\": False,  # Evaluate on the test set after training\n",
    "    \"test_batches\": 100,  # Number of test set batches, -1 uses the entire test set.\n",
    "    \"max_seq_length\": 8192,  # Maximum sequence length.\n",
    "    \"grad_checkpoint\": True,  # Use gradient checkpointing to reduce memory use.\n",
    "    \"lora_parameters\": {\n",
    "        \"keys\": ['mlp.gate_proj', 'mlp.down_proj', 'self_attn.q_proj', 'mlp.up_proj', 'self_attn.o_proj','self_attn.v_proj', 'self_attn.k_proj'],  # The layer keys to apply LoRA to.\n",
    "        \"rank\": 128,  # LoRA rank\n",
    "        \"alpha\": 256,  # LoRA alpha\n",
    "        \"scale\": 10.0,  # LoRA scale\n",
    "        \"dropout\": 0.05  # LoRA dropout\n",
    "    }\n",
    "    # Uncomment to use the lr_schedule\n",
    "    # \"lr_schedule\": {\n",
    "    #     \"name\": \"cosine_decay\",\n",
    "    #     \"warmup\": 100,  # 0 for no warmup\n",
    "    #     \"warmup_init\": 1e-7,  # 0 if not specified\n",
    "    #     \"arguments\": [1e-6, 1000, 1e-7]  # passed to scheduler\n",
    "    # }\n",
    "}\n",
    "\n",
    "# Write the YAML content to the file\n",
    "with open(lora_config, \"w\") as f:\n",
    "    yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08599b49-3455-44af-be4f-5c7aea3a7f6f",
   "metadata": {},
   "source": [
    "## ***Warning!*** Next step starts fine tuning.\n",
    "#### !!Close all unneeded applications before proceeding!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d6cd9-14bf-4917-bfc6-02b338777fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Fine Tuning\n",
    "# !!Close all unneeded applications before proceeding!!\n",
    "# This step wil download the model if not already downloaded\n",
    "\n",
    "!mlx_lm.lora --config={lora_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340421ca-b624-4713-a293-73c05d0ea063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuse the trained model with teh base model\n",
    "\n",
    "os.chdir(model_dir)\n",
    "!mlx_lm.fuse --model {hf_model} --save-path {fused_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b64ce-3d1c-4b6e-9ff3-8b94a4dff011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export a float16 GGUF version of the fine tuned model\n",
    "# https://github.com/ggerganov/llama.cpp\n",
    "\n",
    "!python3 {llamacpp_path}/convert.py {fused_dir} --outtype f16 --outfile {out_gguf} --vocab-type bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24001db-1a1c-4f14-a6ab-b1524f867e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new HF Model Repo\n",
    "\n",
    "api = HfApi()\n",
    "hf_url = api.create_repo(\n",
    "    repo_id = proj_name,\n",
    "    repo_type = \"model\",\n",
    "    private = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f940538-1a61-43ea-9651-2e43babfb4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the created model folder to HuggingFace\n",
    "# This includes the adapters, fused model, and GGUF model\n",
    "\n",
    "api.upload_folder(\n",
    "    repo_id = hf_url.repo_id,\n",
    "    repo_type = \"model\",\n",
    "    folder_path = f\"{proj_dir}model/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d72f77-1313-485a-a972-3481f0b7fc59",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload only the created GGUF file to HF\n",
    "\n",
    "#api.upload_file(\n",
    "#    path_or_fileobj = out_gguf, \n",
    "#    path_in_repo = gguf_file,\n",
    "#    repo_id = hf_url.repo_id,\n",
    "#    repo_type = \"model\"\n",
    "#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
